# Lightweight Food Image Classification via Knowledge Distillation and Attention Mechanisms

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> ğŸ”¥ **è½»é‡çº§é£Ÿå“å›¾åƒåˆ†ç±»**ï¼šé€šè¿‡çŸ¥è¯†è’¸é¦å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©å­¦ç”Ÿæ¨¡å‹è¶…è¶Šæ•™å¸ˆï¼  
> ğŸ“ åŸºäº"å¤§é˜Ÿé•¿æ‰‹æŠŠæ‰‹å¸¦ä½ å‘è®ºæ–‡"æ•™ç¨‹å®ç°

## ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªç»“åˆ**çŸ¥è¯†è’¸é¦**å’Œ**æ³¨æ„åŠ›æœºåˆ¶**çš„è½»é‡çº§é£Ÿå“å›¾åƒåˆ†ç±»æ–¹æ³•ã€‚åœ¨Food-101æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„MobileNetV3å­¦ç”Ÿæ¨¡å‹ï¼ˆé›†æˆECA/SimAMæ³¨æ„åŠ›ï¼‰åœ¨è’¸é¦åè¾¾åˆ°äº†**78.50%**çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ResNet-50æ•™å¸ˆæ¨¡å‹ï¼ˆ76.76%ï¼‰ã€‚

### ğŸŒŸ ä¸»è¦ç‰¹ç‚¹

- âœ… **å¤šç§æ³¨æ„åŠ›æœºåˆ¶**ï¼šæ”¯æŒECAã€SimAMã€CBAMã€SEã€CoordAttentionç­‰
- âœ… **çŸ¥è¯†è’¸é¦æ¡†æ¶**ï¼šæ•™å¸ˆ-å­¦ç”Ÿæ¶æ„ï¼Œæå‡è½»é‡çº§æ¨¡å‹æ€§èƒ½
- âœ… **é«˜æ•ˆè®­ç»ƒ**ï¼šæ··åˆç²¾åº¦è®­ç»ƒã€OneCycleLRå­¦ä¹ ç‡è°ƒåº¦
- âœ… **å®Œæ•´å®éªŒ**ï¼šæ¶ˆèå®éªŒã€æ¨¡å‹å¤æ‚åº¦åˆ†æã€æ¨ç†é€Ÿåº¦æµ‹è¯•
- âœ… **æ˜“äºæ‰©å±•**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ–¹ä¾¿æ·»åŠ æ–°çš„æ³¨æ„åŠ›æœºåˆ¶

### ğŸ“Š ä¸»è¦ç»“æœ

| æ¨¡å‹ | å‚æ•°é‡ | FLOPs | å‡†ç¡®ç‡ | è¯´æ˜ |
|------|--------|-------|--------|------|
| ResNet-50 (Teacher) | 25.6M | 4.1G | 76.76% | æ•™å¸ˆæ¨¡å‹ |
| MobileNetV3-Large | 5.5M | 0.22G | 74.23% | åŸºçº¿æ¨¡å‹ |
| **MobileNetV3 + ECA + KD** | **5.5M** | **0.23G** | **78.50%** | æœ¬æ–‡æ–¹æ³•ï¼ˆæ¨èï¼‰ |
| **MobileNetV3 + SimAM + KD** | **5.5M** | **0.22G** | **78.12%** | æœ¬æ–‡æ–¹æ³• |

> ğŸ‰ å­¦ç”Ÿæ¨¡å‹åœ¨å‚æ•°é‡ä»…ä¸ºæ•™å¸ˆæ¨¡å‹**21.5%**çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®ç‡è¶…è¶Šæ•™å¸ˆ**1.74ä¸ªç™¾åˆ†ç‚¹**ï¼

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ models/                      # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ attention_modules.py    # æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—
â”‚   â””â”€â”€ mobilenetv3_attention.py # MobileNetV3 + æ³¨æ„åŠ›
â”œâ”€â”€ utils/                       # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model_complexity.py     # æ¨¡å‹å¤æ‚åº¦åˆ†æ
â”œâ”€â”€ train_distillation.py        # çŸ¥è¯†è’¸é¦è®­ç»ƒè„šæœ¬
â”œâ”€â”€ run_ablation_study.sh        # æ¶ˆèå®éªŒè„šæœ¬
â”œâ”€â”€ requirements.txt             # ä¾èµ–åŒ…
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â””â”€â”€ paper/                       # è®ºæ–‡ç›¸å…³ï¼ˆLaTeXæºç ï¼‰
    â””â”€â”€ paper.tex
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/blackwhitez246/lightweight-food-classification.git
cd lightweight-food-classification

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
conda create -n food_cls python=3.8
conda activate food_cls

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡

ä¸‹è½½Food-101æ•°æ®é›†ï¼š

```bash
# æ–¹æ³•1: ä½¿ç”¨torchvisionè‡ªåŠ¨ä¸‹è½½
python -c "from torchvision import datasets; datasets.Food101(root='./data', download=True)"

# æ–¹æ³•2: æ‰‹åŠ¨ä¸‹è½½
# ä¸‹è½½åœ°å€: https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/
# è§£å‹åˆ° ./data/food-101/
```

### 3. è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æœ‰é¢„è®­ç»ƒçš„ResNet-50æ•™å¸ˆæ¨¡å‹ï¼Œå¯ä»¥è·³è¿‡æ­¤æ­¥éª¤ã€‚å¦åˆ™ï¼š

```bash
python train_teacher.py \
    --data-dir ./data \
    --epochs 30 \
    --batch-size 64 \
    --output teacher_resnet50.pth
```

### 4. è®­ç»ƒå­¦ç”Ÿæ¨¡å‹

#### 4.1 ä½¿ç”¨ECAæ³¨æ„åŠ› + çŸ¥è¯†è’¸é¦

```bash
python train_distillation.py \
    --data-dir ./data \
    --attention-type eca \
    --teacher-checkpoint teacher_resnet50.pth \
    --temperature 4.0 \
    --alpha 0.7 \
    --epochs 30 \
    --batch-size 64 \
    --lr 0.05 \
    --pretrained \
    --output-dir ./checkpoints/eca
```

#### 4.2 ä½¿ç”¨SimAMæ³¨æ„åŠ› + çŸ¥è¯†è’¸é¦

```bash
python train_distillation.py \
    --data-dir ./data \
    --attention-type simam \
    --teacher-checkpoint teacher_resnet50.pth \
    --temperature 4.0 \
    --alpha 0.7 \
    --epochs 30 \
    --batch-size 64 \
    --lr 0.05 \
    --pretrained \
    --output-dir ./checkpoints/simam
```

### 5. æ¨¡å‹å¤æ‚åº¦åˆ†æ

```bash
python utils/model_complexity.py
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
========================================
æ¨¡å‹å¤æ‚åº¦åˆ†æç»“æœ
========================================
æ€»å‚æ•°é‡:        5,483,237 (5.48M)
å¯è®­ç»ƒå‚æ•°:      5,483,237
FLOPs:           219.909M (0.22 G)
æ¨¡å‹å¤§å°:        20.92 MB
æ¨ç†æ—¶é—´:        3.45 Â± 0.12 ms
ååé‡:          289.86 images/s
========================================
```

### 6. æ¶ˆèå®éªŒ

è¿è¡Œå®Œæ•´çš„æ¶ˆèå®éªŒï¼ˆæµ‹è¯•ä¸åŒæ³¨æ„åŠ›æœºåˆ¶å’Œè®­ç»ƒç­–ç•¥ï¼‰ï¼š

```bash
chmod +x run_ablation_study.sh
./run_ablation_study.sh
```

---

## ğŸ“ˆ å®éªŒç»“æœ

### æ¶ˆèå®éªŒ

| å®éªŒé…ç½® | æ³¨æ„åŠ› | è’¸é¦ | å‡†ç¡®ç‡ | å‚æ•°é‡ |
|----------|--------|------|--------|--------|
| Baseline | âŒ | âŒ | 74.23% | 5.48M |
| +ECA | âœ… | âŒ | 75.86% | 5.48M |
| +SimAM | âœ… | âŒ | 75.42% | 5.48M |
| +Distillation | âŒ | âœ… | 76.91% | 5.48M |
| **+ECA +KD (å®Œæ•´æ–¹æ³•)** | **âœ…** | **âœ…** | **78.50%** | **5.48M** |
| **+SimAM +KD (å®Œæ•´æ–¹æ³•)** | **âœ…** | **âœ…** | **78.12%** | **5.48M** |

### ä¸åŒæ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”

| æ³¨æ„åŠ›æœºåˆ¶ | å‚æ•°é‡ | FLOPs | å‡†ç¡®ç‡ | ç‰¹ç‚¹ |
|------------|--------|-------|--------|------|
| **ECA** | 5.48M | 0.22G | **78.50%** | æ— é™ç»´ã€å±€éƒ¨äº¤äº’ |
| **SimAM** | 5.48M | 0.22G | **78.12%** | æ— å‚æ•°ã€èƒ½é‡å‡½æ•° |
| CBAM | 5.52M | 0.23G | 77.89% | ä¸²è”é€šé“+ç©ºé—´ |
| SE | 5.51M | 0.22G | 77.65% | ç»å…¸é€šé“æ³¨æ„åŠ› |
| CoordAttention | 5.49M | 0.23G | 77.92% | ä½ç½®ç¼–ç  |

### è®­ç»ƒæ›²çº¿

![è®­ç»ƒæ›²çº¿](assets/training_curves.png)

---

## ğŸ”¬ æ–¹æ³•è¯¦è§£

### 1. æ³¨æ„åŠ›æœºåˆ¶

#### ECA (Efficient Channel Attention)
- **ç‰¹ç‚¹**ï¼šä¸é™ç»´çš„å±€éƒ¨è·¨é€šé“äº¤äº’
- **ä¼˜åŠ¿**ï¼šå‚æ•°å°‘ã€æ•ˆæœå¥½
- **å®ç°**ï¼š1Då·ç§¯è‡ªé€‚åº”æ•è·é€šé“ä¾èµ–

```python
from models import get_attention_module

eca = get_attention_module('eca', channels=64)
output = eca(input_tensor)
```

#### SimAM (Simple Parameter-Free Attention Module)
- **ç‰¹ç‚¹**ï¼šåŸºäºèƒ½é‡å‡½æ•°çš„3Dæ³¨æ„åŠ›
- **ä¼˜åŠ¿**ï¼šé›¶å‚æ•°ã€å³æ’å³ç”¨
- **å®ç°**ï¼šé€šè¿‡ç¥ç»å…ƒä¸é‚»åŸŸçš„èƒ½é‡å·®å¼‚å»ºæ¨¡

```python
simam = get_attention_module('simam')
output = simam(input_tensor)
```

### 2. çŸ¥è¯†è’¸é¦

æŸå¤±å‡½æ•°ï¼š

```
L = Î± * L_CE(y, p_student) + (1-Î±) * TÂ² * KL(p_teacher^T || p_student^T)
```

å…¶ä¸­ï¼š
- `L_CE`: ç¡¬æ ‡ç­¾äº¤å‰ç†µæŸå¤±
- `KL`: KLæ•£åº¦ï¼ˆè½¯æ ‡ç­¾æŸå¤±ï¼‰
- `T`: æ¸©åº¦ç³»æ•°ï¼ˆé»˜è®¤4.0ï¼‰
- `Î±`: å¹³è¡¡ç³»æ•°ï¼ˆé»˜è®¤0.7ï¼‰

---

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æ³¨æ„åŠ›æœºåˆ¶

åœ¨`models/attention_modules.py`ä¸­æ·»åŠ æ–°çš„æ³¨æ„åŠ›æ¨¡å—ï¼š

```python
class MyAttention(nn.Module):
    def __init__(self, channels):
        super().__init__()
        # ä½ çš„å®ç°
    
    def forward(self, x):
        # ä½ çš„å®ç°
        return x

# æ³¨å†Œåˆ°å­—å…¸
ATTENTION_MODULES['my_attention'] = MyAttention
```

### åœ¨å…¶ä»–æ•°æ®é›†ä¸Šè®­ç»ƒ

æœ¬é¡¹ç›®æ”¯æŒä»»ä½•ImageFolderæ ¼å¼çš„æ•°æ®é›†ï¼š

```bash
python train_distillation.py \
    --data-dir /path/to/your/dataset \
    --attention-type eca \
    --epochs 50 \
    --batch-size 32
```

æ•°æ®é›†ç›®å½•ç»“æ„ï¼š
```
your_dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ class1/
â”‚   â”œâ”€â”€ class2/
â”‚   â””â”€â”€ ...
â””â”€â”€ val/
    â”œâ”€â”€ class1/
    â”œâ”€â”€ class2/
    â””â”€â”€ ...
```

### å¯¼å‡ºä¸ºONNX

```python
import torch
from models import create_model

model = create_model(num_classes=101, attention_type='eca')
model.load_state_dict(torch.load('best_model.pth'))
model.eval()

dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(model, dummy_input, "model.onnx", 
                  opset_version=11, input_names=['input'], 
                  output_names=['output'])
```

---

## ğŸ“ å¼•ç”¨

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{yourname2025lightweight,
  title={Lightweight Food Image Classification via Knowledge Distillation and Attention Mechanisms},
  author={Your Name},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
```

### ç›¸å…³è®ºæ–‡

- **ECA-Net**: Wang et al., "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks", CVPR 2020
- **SimAM**: Yang et al., "SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks", ICML 2021
- **Knowledge Distillation**: Hinton et al., "Distilling the Knowledge in a Neural Network", arXiv 2015
- **MobileNetV3**: Howard et al., "Searching for MobileNetV3", ICCV 2019

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

1. Forkæœ¬ä»“åº“
2. åˆ›å»ºä½ çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤ä½ çš„æ”¹åŠ¨ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ä¸€ä¸ªPull Request

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢ [å¤§é˜Ÿé•¿](https://space.bilibili.com/3493095297518401) çš„è¯¦ç»†æ•™ç¨‹
- æ„Ÿè°¢ PyTorch å›¢é˜Ÿæä¾›çš„ä¼˜ç§€æ¡†æ¶
- æ„Ÿè°¢ Food-101 æ•°æ®é›†çš„ä½œè€…

---

## ğŸ“® è”ç³»æ–¹å¼

- **ä½œè€…**: Alex Zander
- **Email**: 21011149@mail.ecust.edu.cn
- **ä¸»é¡µ**: https://github.com/blackwhitez246

---

## â­ Star History

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸€ä¸ªStar â­ï¼

[![Star History Chart](https://api.star-history.com/svg?repos=blackwhitez246/lightweight-food-classification&type=Date)](https://star-history.com/#blackwhitez246/lightweight-food-classification&Date)

---

<p align="center">
  Made with â¤ï¸ by <a href="https://github.com/blackwhitez246">Alex Zander</a>
</p>


